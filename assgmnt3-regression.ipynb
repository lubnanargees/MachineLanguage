{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ee4c67c-f88c-4cfb-9d60-75918f06c762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression Assignment using California Housing Dataset\n",
    "\n",
    "# 1. Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a891d25-377f-4568-8d1a-84087bbcc873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load the dataset\n",
    "housing = fetch_california_housing()\n",
    "X = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "y = pd.Series(housing.target, name='MedHouseValue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55804d3c-a262-40ca-b76d-ec973acb3484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
      "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
      "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
      "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
      "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
      "\n",
      "   Longitude  \n",
      "0    -122.23  \n",
      "1    -122.22  \n",
      "2    -122.24  \n",
      "3    -122.25  \n",
      "4    -122.25  \n"
     ]
    }
   ],
   "source": [
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1776b14a-b6f1-415e-b60c-44536d933cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    4.526\n",
      "1    3.585\n",
      "2    3.521\n",
      "3    3.413\n",
      "4    3.422\n",
      "Name: MedHouseValue, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85762f43-5b1e-4c7c-b88c-e22e0f7a5379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in features:\n",
      " MedInc        0\n",
      "HouseAge      0\n",
      "AveRooms      0\n",
      "AveBedrms     0\n",
      "Population    0\n",
      "AveOccup      0\n",
      "Latitude      0\n",
      "Longitude     0\n",
      "dtype: int64\n",
      "Missing values in target:\n",
      " 0\n"
     ]
    }
   ],
   "source": [
    "# 3. Check for missing values\n",
    "print(\"Missing values in features:\\n\", X.isnull().sum())\n",
    "print(\"Missing values in target:\\n\", y.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b9c9e2b-fbd3-4daf-b4e3-e3256a387e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# 4. Feature Scaling - Standardization\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "860ae9f3-5d60-47fb-8010-c453b420b7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 rows of scaled features:\n",
      "      MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "0  2.344766  0.982143  0.628559  -0.153758   -0.974429 -0.049597  1.052548   \n",
      "1  2.332238 -0.607019  0.327041  -0.263336    0.861439 -0.092512  1.043185   \n",
      "2  1.782699  1.856182  1.155620  -0.049016   -0.820777 -0.025843  1.038503   \n",
      "3  0.932968  1.856182  0.156966  -0.049833   -0.766028 -0.050329  1.038503   \n",
      "4 -0.012881  1.856182  0.344711  -0.032906   -0.759847 -0.085616  1.038503   \n",
      "\n",
      "   Longitude  \n",
      "0  -1.327835  \n",
      "1  -1.322844  \n",
      "2  -1.332827  \n",
      "3  -1.337818  \n",
      "4  -1.337818  \n"
     ]
    }
   ],
   "source": [
    "# Display first 5 rows of the scaled data\n",
    "print(\"\\nFirst 5 rows of scaled features:\\n\", X_scaled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72541dc9-3716-44d8-ac05-e04b264ee788",
   "metadata": {},
   "source": [
    "üìù Explanation of Preprocessing Steps:\n",
    "    1. Loading the Dataset:Used fetch_california_housing() from sklearn.datasets to load the data, which includes various housing-related features and the median house value (target variable).\n",
    "    2. Conversion to pandas DataFrame: Converted the dataset to a DataFrame for easier analysis, visualization, and preprocessing.\n",
    "    3. Handling Missing Values: Used isnull().sum() to check for missing values. (Note: This dataset does not contain missing values, but it‚Äôs good practice to always check.)\n",
    "    4. Feature Scaling (Standardization): Applied StandardScaler to scale the features so that they all have a mean of 0 and a standard deviation of 1.This step is important because many regression models (especially linear ones) perform better when features are on a similar scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51eb1164-e374-4c9c-b0a1-09289c79ba56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Linear Regression\n",
      "-----------------\n",
      "Mean Squared Error: 0.5559\n",
      "R-squared: 0.5758\n",
      "\n",
      "Decision Tree Regressor\n",
      "-----------------------\n",
      "Mean Squared Error: 0.4943\n",
      "R-squared: 0.6228\n",
      "\n",
      "Random Forest Regressor\n",
      "-----------------------\n",
      "Mean Squared Error: 0.2555\n",
      "R-squared: 0.8050\n",
      "\n",
      "Gradient Boosting Regressor\n",
      "---------------------------\n",
      "Mean Squared Error: 0.2940\n",
      "R-squared: 0.7756\n",
      "\n",
      "Support Vector Regressor (SVR)\n",
      "------------------------------\n",
      "Mean Squared Error: 0.3552\n",
      "R-squared: 0.7289\n"
     ]
    }
   ],
   "source": [
    "# Required libraries\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Store models and their names\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Decision Tree Regressor\": DecisionTreeRegressor(random_state=42),\n",
    "    \"Random Forest Regressor\": RandomForestRegressor(random_state=42),\n",
    "    \"Gradient Boosting Regressor\": GradientBoostingRegressor(random_state=42),\n",
    "    \"Support Vector Regressor (SVR)\": SVR()\n",
    "}\n",
    "\n",
    "# Evaluate each model\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"\\n{name}\")\n",
    "    print(\"-\" * len(name))\n",
    "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "    print(f\"R-squared: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1196874e-417c-4382-b439-99e98e3fcc8a",
   "metadata": {},
   "source": [
    "üìù Explanations and Suitability of Each Algorithm\n",
    "1. Linear Regression\n",
    "How it works: It models the relationship between the dependent and independent variables by fitting a straight line.\n",
    "\n",
    "Suitability: Good baseline model; useful for identifying linear relationships. Assumes the features are linearly related to the target.\n",
    "\n",
    "2. Decision Tree Regressor\n",
    "How it works: Splits the data based on feature thresholds to minimize prediction error in each branch.\n",
    "\n",
    "Suitability: Handles non-linear relationships and doesn‚Äôt require feature scaling. May overfit, but interpretable.\n",
    "\n",
    "3. Random Forest Regressor\n",
    "How it works: An ensemble of decision trees where each tree is trained on a random subset of the data and features.\n",
    "\n",
    "Suitability: More accurate and robust than individual trees. Handles non-linear data and reduces overfitting.\n",
    "\n",
    "4. Gradient Boosting Regressor\n",
    "How it works: Builds models sequentially; each new model corrects the errors of the previous one using gradient descent.\n",
    "\n",
    "Suitability: Often delivers state-of-the-art performance; great for structured/tabular data like this.\n",
    "\n",
    "5. Support Vector Regressor (SVR)\n",
    "How it works: Tries to fit the best line (or hyperplane) within a margin, using kernel tricks for non-linear data.\n",
    "\n",
    "Suitability: Effective in high-dimensional spaces and can capture complex patterns, but slower on large datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "348a6fa4-39c2-41b6-a5f9-72d87681840d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Evaluation Summary:\n",
      "\n",
      "                            Model       MSE       MAE  R¬≤ Score\n",
      "2         Random Forest Regressor  0.255498  0.327613  0.805024\n",
      "3     Gradient Boosting Regressor  0.293999  0.371650  0.775643\n",
      "4  Support Vector Regressor (SVR)  0.355198  0.397763  0.728941\n",
      "1         Decision Tree Regressor  0.494272  0.453784  0.622811\n",
      "0               Linear Regression  0.555892  0.533200  0.575788\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Dictionary to store evaluation results\n",
    "results = {\n",
    "    \"Model\": [],\n",
    "    \"MSE\": [],\n",
    "    \"MAE\": [],\n",
    "    \"R¬≤ Score\": []\n",
    "}\n",
    "\n",
    "# Re-run models and collect metrics\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results[\"Model\"].append(name)\n",
    "    results[\"MSE\"].append(mse)\n",
    "    results[\"MAE\"].append(mae)\n",
    "    results[\"R¬≤ Score\"].append(r2)\n",
    "\n",
    "# Create a DataFrame for easy comparison\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by=\"R¬≤ Score\", ascending=False)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nModel Evaluation Summary:\\n\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76a28e3-b093-473a-96bc-76edd2a3c0b1",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "After running the above code, you can analyze the results and write the following conclusions manually based on output:\n",
    "-- Best-Performing Algorithm:\n",
    "Likely to be Gradient Boosting Regressor or Random Forest Regressor\n",
    "\n",
    "Justification: These models usually achieve higher R¬≤ and lower error metrics due to their ensemble learning techniques that reduce bias and variance.\n",
    "\n",
    "-- Worst-Performing Algorithm:\n",
    "Often Linear Regression or SVR\n",
    "\n",
    "Reasoning:\n",
    "\n",
    "Linear Regression might underperform due to inability to capture non-linear relationships.\n",
    "\n",
    "SVR can struggle with large datasets and sensitive hyperparameters, causing poorer generalization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95838da-429e-48ab-9f95-90d038b0b46f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
